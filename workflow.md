## Maudlin Workflow Overview

### **Training Workflow**

1. **Read CSV Data:**

   - Load input data from CSV files specified in the configuration.

2. **Featurization:**

   1. Apply feature engineering steps by running featurizers in the order defined in the configuration.

3. Target generation

   1. For each row in the subset DataFrame:
      1. Call a unit-specific target function to compute the target (ground truth).

4) **Subset Columns:**

   - Subset the original columns as needed for preprocessing, retaining only those required or generated during featurization.

5) **Input Generation:**

   - For each row in the subset DataFrame:
     - Call a **unit-specific input function** to create the input row for the model.

6) Split Data

   1. Split data into X and Y, test, train, and validation sets.

7) **Pre-Training Stage:**

   - Apply oversampling techniques like **SMOTE** or **ADASYN** based on configuration.
   - Perform **PCA** either before or after oversampling, as configured.
   - Generate pre-training visualizations:
     - SMOTE/ADASYN and PCA plots.
     - Correlation matrix.
     - Box plots.
     - Pair plots (may require significant computation time).
   - Execute **unit-specific pre-training functions** for additional preprocessing.

8) **Model Creation:**

   - Define a new model for each training run based on the `model_architecture` setting in the configuration.
   - Apply **class weights** to address data imbalance.

9) **Training Execution:**

   - Train the model using **Keras** with `model.fit()`, which iterates through configured epochs and batches.

10) **Post-Training Stage:**

    - Perform post-training tasks that depend on model predictions (thresholds are applied).
    - Generate post-training visualizations:
      - **Confusion Matrix.**
      - **Precision-Recall Curve.**
      - **ROC Curve.**
      - **SHAP** visualizations (may require significant computation time).
    - Execute **unit-specific post-training functions** for additional processing.

11) **Results Storage:**

    - Each training run generates a directory containing:
      - The config file
      - The model file
      - Model metrics in JSON format.
      - Generated visualizations for analysis and comparison.

12) **Iterative Refinement:**

    - Use **mdln history** to review previous runs and metrics.
    - Adjust configurations based on results and repeat training on a new model.

---

### **Prediction Workflow**

1. **Read CSV Data:**

   - Load prediction data from the configured CSV file.

2. Apply perturbations

   1. Apply configured perturbations to specified features, such as:
      1. Numeric ranges.
      2. Binary switches.

3. **Featurization:**

   1. Apply the same feature engineering steps as during training.

4. **Subset Columns:**

   - Subset the original columns as needed, retaining only those required or generated during featurization.

5. **Input Generation:**

   - For each row in the subset DataFrame:
     - Call a **unit-specific input function** to create the input row for prediction.
   - No targets are generated

6. Load Model from the last/currently selected training run

7. **Pre-Prediction Stage:**

   - Generate images
   - Execute **unit-specific pre-prediction functions** if defined.
   - Note: No pre-prediction visualizations are currently generated.

8. **Prediction Execution:**

   - Use `model.predict(X_pca)` to generate predictions.
   - Apply configured thresholds to classify results if required.

9. **Post-Prediction Stage:**

   - Generate visualizations based on predictions:
     - **Calibration Curve.**
     - **Prediction Histogram.**
     - **Line Plot.**
     - **Scatter Plot.**
   - Execute **unit-specific post-prediction functions** for additional processing.

10. **Results Storage:**

    - Save prediction outputs, metrics, and visualizations to the run directory.

---

### **Experiment Tracking and Iteration**

- Each training and prediction run generates a unique directory containing:
  - Configuration files.
  - Metrics in JSON format.
  - Visualizations for further evaluation.
- The training run will store the model file generated by Keras.
- Use the command **mdln history** to track and compare runs.
- Refine configurations iteratively to improve model performance.

This structured workflow ensures consistent experimentation, enabling rapid testing and evaluation of different configurations and features.

